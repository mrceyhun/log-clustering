#!/bin/sh
# Author: Valentin Kuznetsov <vkuznet AT gmail [DOT] com>
# A wrapper script to submit spark job with python script.

# Kerberos
keytab=/etc/secrets/keytab
if [ -f $keytab ]; then
    principal=`klist -k "$keytab" | tail -1 | awk '{print $2}'`
    echo "principal=$principal"
    kinit $principal -k -t "$keytab"
    if [ $? == 1 ]; then
        echo "Unable to perform kinit"
        exit 1
    fi
    klist -k "$keytab"
fi

# add non-standard location of hadoop
export PATH=$PATH:/usr/hdp/spark/bin

# test arguments
if [ "$#" -eq 0 ]; then
    echo "Usage: run_spark <pyspark_script> <options>"
    exit 1
fi

cmsspark=$1

# enable simple secret to run in non-yarn mode
conf="--conf spark.hadoop.dfs.client.socket-timeout=120000"
hostname=`hostname -s`
conf="$conf --conf spark.authenticate.secret=cmsspark --conf spark.yarn.security.credentials.hive.enabled=false --conf spark.driver.port=5001 --conf spark.blockManager.port=5101 --conf spark.ui.port=5201"

# look if we requested to show full log output, to disable spark output
# client should setup his/her own log4j.properties via WMA_LOG4J environment variable
if [[ "$@" =~ "--no-log4j" ]]; then
    conf=" --conf spark.ui.showConsoleProgress=false "
    if [ -n "$LOG4J_CONF" ] && [ -f $LOG4J_CONF ]; then
        conf="$conf --conf spark.driver.extraJavaOptions='-Dlog4j.configuration=file:$LOG4J_CONF'"
    fi
fi

# read spark options from configuration file if it exists
# the SPARK_OPTIONS file should provide <option=value> line for every option
# spark.network.timeout=120s
# spark.rpc.numRetries=3
if [ -n "$SPARK_OPTIONS" ]; then
    conf="$conf `cat $SPARK_OPTIONS | awk '{ORS=" "; print "--conf "$0""}'`"
fi

# from https://cern.service-now.com/service-portal/view-request.do?n=RQF0876659
# we can specify in jar files as following (before Spark 2.0):
# --packages com.databricks:spark-csv_2.11:1.6.0

# set avro jars
echo "Using spark 2.X"
jars=""
conf="$conf --packages com.databricks:spark-avro_2.11:4.0.0"

args="${@:2}"
echo "$cmsspark $args"
yarn="--yarn"

if [ "$2" == "-h" ] || [ "$2" == "--help" ] || [ "$2" == "-help" ]; then
    # run help
    python3 $cmsspark --help
    exit 0
fi
echo "PYTHONPATH: $PYTHONPATH"
echo "cmsspark: $cmsspark $args"
# to tune up these numbers:
#  - executor-memory not more than 5G
#  - num-executor can be increased (suggested not more than 10)
#  - cores = 2/4/8
# Temp solution to have a wrapper for python27 on spark cluster
# once CERN IT will resolve python version we can remove PYSPARK_PYTHON
echo "YARN execution: $conf"
spark-submit $jars \
    --master yarn \
    --executor-memory 5g \
    $conf $cmsspark $args
